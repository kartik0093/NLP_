{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_v9-B89wEonm",
    "outputId": "40b3ce4e-bf43-45c9-dc4b-b4b9eeda9bfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GejZH-BZq7v",
    "outputId": "ff0690a5-582e-4175-9015-066062d4d58e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbrJhIWSZRtg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xt7zrqkIaeuS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6iwtVcHarlp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB Dataset.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "a9JWlMJEaroH",
    "outputId": "8330f315-7a93-4e23-ce4e-c2858fd7e7e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zsu-yYBarx2",
    "outputId": "beb046f3-700c-4ed7-dd32-f7c586be9ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has 50000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print('File has {} rows and {} columns'.format(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XA-F_az1bA08"
   },
   "outputs": [],
   "source": [
    "import transformers  \n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKBazp9xbA32",
    "outputId": "c5381062-db04-459c-f0ca-c769be1cc544"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(data.sentiment.unique())\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk2rbA-zbTkP"
   },
   "outputs": [],
   "source": [
    "sent= 'how to train the model, lets look at how a trained model calculates its prediction.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubMImrjyb2Xy"
   },
   "outputs": [],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbkltrLqbTnM",
    "outputId": "64959b15-59b0-43ec-9d2a-1d41c8d6c059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tokenized_sequence= bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =30,pad_to_max_length = True,return_attention_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oanAtID8bpMG",
    "outputId": "af80226e-a011-4a42-c5c2-ad9def479ad9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2129, 2000, 3345, 1996, 2944, 1010, 11082, 2298, 2012, 2129, 1037, 4738, 2944, 18422, 2015, 2049, 17547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "_FlkiRSCbpOw",
    "outputId": "3ff73c51-b985-4f80-da59-93d54a3ac3e8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] how to train the model, lets look at how a trained model calculates its prediction. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(tokenized_sequence['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "MUDP2h_3bpRn",
    "outputId": "ef3172c6-bf0c-4b00-de9e-063b1c1eadf8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      0\n",
       "1  A wonderful little production. <br /><br />The...  positive      0\n",
       "2  I thought this was a wonderful way to spend ti...  positive      0\n",
       "3  Basically there's a family where a little boy ...  negative      1\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = data['sentiment'].map({'positive':0,'negative':1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giHNlvcjcTve",
    "outputId": "c6b46271-ae8c-450f-80c4-decb8b84cda0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=data['review']\n",
    "labels=data['label']\n",
    "len(sentences),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdLO_1Aecdsm"
   },
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in sentences:\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =256,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLG01qDXcd0y",
    "outputId": "435512dc-6a3e-48da-d708-9b8b84107d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids),len(attention_masks),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sb-9xv1Wcd3T",
    "outputId": "7eb71f78-9a5d-4c7e-fee0-f9f639da4bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the pickle file.....\n"
     ]
    }
   ],
   "source": [
    "print('Preparing the pickle file.....')\n",
    "\n",
    "pickle_inp_path='/content/drive/MyDrive/data.pkl'\n",
    "pickle_mask_path='/content/drive/MyDrive/data.pkl'\n",
    "pickle_label_path='/content/drive/MyDrive/data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDJyLKa0eAWO",
    "outputId": "d261bf6e-5088-4868-f283-dc2a068ad994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle files saved as  /content/drive/MyDrive/data.pkl /content/drive/MyDrive/data.pkl /content/drive/MyDrive/data.pkl\n"
     ]
    }
   ],
   "source": [
    "pickle.dump((input_ids),open(pickle_inp_path,'wb'))\n",
    "pickle.dump((attention_masks),open(pickle_mask_path,'wb'))\n",
    "pickle.dump((labels),open(pickle_label_path,'wb'))\n",
    "\n",
    "\n",
    "print('Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XwBsVSneAY7",
    "outputId": "d4c04b30-c149-488c-c78e-88d578d73dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the saved pickle files..\n",
      "Input shape (50000,) Attention mask shape (50000,) Input label shape (50000,)\n"
     ]
    }
   ],
   "source": [
    "print('Loading the saved pickle files..')\n",
    "\n",
    "input_ids=pickle.load(open(pickle_inp_path, 'rb'))\n",
    "attention_masks=pickle.load(open(pickle_mask_path, 'rb'))\n",
    "labels=pickle.load(open(pickle_label_path, 'rb'))\n",
    "\n",
    "print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49n6Yt-3eAbr",
    "outputId": "cdfdc600-ca7c-4ab8-8bf2-c6a053a636fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inp shape (40000,) Val input shape (10000,)\n",
      "Train label shape (40000,) Val label shape (10000,)\n",
      "Train attention mask shape (40000,) Val attention mask shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)\n",
    "\n",
    "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOqIUhF0eRdw"
   },
   "outputs": [],
   "source": [
    "model_save_path='/content/drive/MyDrive/bert_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTPyNAiueRgp"
   },
   "outputs": [],
   "source": [
    "log_dir='/content/drive/MyDrive'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFall1ekeRmT",
    "outputId": "b84b5bea-95a8-4f82-ae64-f8efd9f2ab9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_5jbwCDcTyK",
    "outputId": "d8692700-f9cf-4819-ae73-85ad26a94faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1250/1250 [==============================] - 148s 109ms/step - loss: 0.0583 - accuracy: 0.9632 - val_loss: 7.6469e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "1250/1250 [==============================] - 133s 106ms/step - loss: 9.0556e-05 - accuracy: 1.0000 - val_loss: 2.3007e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "1250/1250 [==============================] - 133s 106ms/step - loss: 3.2441e-05 - accuracy: 1.0000 - val_loss: 1.0312e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "1250/1250 [==============================] - 133s 107ms/step - loss: 1.5034e-05 - accuracy: 1.0000 - val_loss: 5.2452e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history=bert_model.fit([train_inp,train_mask],train_label,batch_size=32,epochs=4,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDr6swVYgQYh"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiGt6dCkgQbV"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwiwJZWhi_Vg",
    "outputId": "533024bd-5a32-458b-9db5-8de04b9ce706"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 1.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4996\n",
      "           1       1.00      1.00      1.00      5004\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "trained_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights('/content/drive/MyDrive/bert_model.h5')\n",
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)\n",
    "pred_labels = np.argmax(preds.logits, axis=1)\n",
    "f1 = f1_score(val_label,pred_labels)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "print(classification_report(val_label,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ci3GMZGHj5Q3"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUo3Q73iklPk"
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "c594e3c18d874b4eb5bedc1037948331",
      "547234639c33424eaf91bb4588e7463a"
     ]
    },
    "id": "39OereeGloQV",
    "outputId": "25641305-2ef6-4f29-e8c6-668b108ddc66"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c594e3c18d874b4eb5bedc1037948331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547234639c33424eaf91bb4588e7463a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_209']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8_mUM4DrLYJ"
   },
   "outputs": [],
   "source": [
    "model_save_path='/content/drive/MyDrive/distilbert_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LwKkQ00rI3-"
   },
   "outputs": [],
   "source": [
    "log_dir='/content/drive/MyDrive'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-PZ2-heloYC",
    "outputId": "a60f53a6-458d-407d-c8c8-8a76075ba91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "model.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N25kFTdvoktk",
    "outputId": "6522ff73-d0b4-4879-97d9-7d4229e10d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 82s 66ms/step - loss: 1.0784e-06 - accuracy: 1.0000 - val_loss: 1.7896e-07 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 82s 65ms/step - loss: 4.5955e-07 - accuracy: 1.0000 - val_loss: 5.9652e-08 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model2=model.fit([train_inp,train_mask],train_label,batch_size=32,epochs=2,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-kf065pqAeP"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s94NC25HqAhJ",
    "outputId": "a5fa4a5f-26b3-4c12-bfd4-23b3667ae6c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_229']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 1.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4996\n",
      "           1       1.00      1.00      1.00      5004\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "trained_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights('/content/drive/MyDrive/distilbert_model.h5')\n",
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)\n",
    "pred_labels = np.argmax(preds.logits, axis=1)\n",
    "f1 = f1_score(val_label,pred_labels)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "print(classification_report(val_label,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYJk9mCeqAkc"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLL5STUSriMX"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Asza8LB11Fv_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8znB8_wriPr"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUMfi5fzuZUU"
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBCAOx4ex7n_",
    "outputId": "7bf29751-1ce5-4fb9-d042-0214c114314f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "tokenizer = TFAutoModelForSequenceClassification.from_pretrained('roberta-base')\n",
    "model3 = TFAutoModelForSequenceClassification.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8UcV02Gs8uX"
   },
   "outputs": [],
   "source": [
    "model_save_path='/content/drive/MyDrive/robertabert_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XUCo1trs_s6"
   },
   "outputs": [],
   "source": [
    "log_dir='/content/drive/MyDrive'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMB8ddHps_0h",
    "outputId": "4e06d181-9ec0-475e-9134-56bd62ee45fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  592130    \n",
      "=================================================================\n",
      "Total params: 124,647,170\n",
      "Trainable params: 124,647,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "model3.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3o5Z59Ss_9M",
    "outputId": "6ee718c0-f664-44a4-e61b-6d51e18d2033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 163s 122ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 2.1280e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 140s 112ms/step - loss: 3.2272e-05 - accuracy: 1.0000 - val_loss: 6.4971e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model3=model3.fit([train_inp,train_mask],train_label,batch_size=32,epochs=2,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxbzVwAK0vWI",
    "outputId": "b9cee664-8d12-435d-ed92-4c3c1e64d0b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 1.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4996\n",
      "           1       1.00      1.00      1.00      5004\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "trained_model = TFAutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels=2)\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights('/content/drive/MyDrive/robertabert_model.h5')\n",
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)\n",
    "pred_labels = np.argmax(preds.logits, axis=1)\n",
    "f1 = f1_score(val_label,pred_labels)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "print(classification_report(val_label,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVIVWuKp5Cs-",
    "outputId": "422a6fed-b947-466f-e315-5c04e29a0158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PXi3UUX0vNT"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "tokenizer = TFAutoModelForSequenceClassification.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpQo9Kva3qO-",
    "outputId": "931cd928-8726-4361-e29c-8b01a5a1cd56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, XLNetModel\n",
    "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
    "tokenizer = TFAutoModelForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy9Bx_5c8hoX",
    "outputId": "0e560f03-3e0b-422f-9aff-82805535acfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "xlnet = TFAutoModelForSequenceClassification.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyZuDZ0X8h0-"
   },
   "outputs": [],
   "source": [
    "model_save_path='/content/drive/MyDrive/XLnet_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIuklqH99r7J"
   },
   "outputs": [],
   "source": [
    "log_dir='/content/drive/MyDrive'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4yZklNG9r-U",
    "outputId": "915265c7-18d2-4b5d-ded0-d8887b9df718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfxl_net_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFXLNetMainLaye multiple                  116718336 \n",
      "_________________________________________________________________\n",
      "sequence_summary (TFSequence multiple                  590592    \n",
      "_________________________________________________________________\n",
      "logits_proj (Dense)          multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 117,310,466\n",
      "Trainable params: 117,310,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "xlnet.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "xlnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XfU2mUN9sAW",
    "outputId": "8c4ad88a-8215-4d25-cff8-b27cc123597f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification_1/transformer/mask_emb:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification_1/transformer/mask_emb:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_for_sequence_classification_1/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss.\n",
      "1250/1250 [==============================] - 152s 112ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 4.0531e-06 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.6552e-05 - accuracy: 1.0000 - val_loss: 1.6094e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model4=xlnet.fit([train_inp,train_mask],train_label,batch_size=32,epochs=2,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYzn-mMbFxQO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "je66-Id2FxTn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYSFEzCRFxX0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DATATOBIZ_TASK_DAY_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
